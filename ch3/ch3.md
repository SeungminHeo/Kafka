카프카 디자인
---
실시간 데이터 피드를 효율적으로 처리하는 통합 플랫폼의 역할. 
높은 처리량, 빠른 메시지 전송, 운영효율화를 위한 시스템, 페이지 캐싱, 배치 전송처리 등의 구현

1. 분산 시스템
    - 높은 성능
    - 장애처리 용이
    - 확장성, 단순히 브로커 수를 높이기만 하면 됨.(cf. 링크드인의 경우 최대 60개.) 
2. 페이지 캐시: 처리량을 높이기 위한 기능
    - 운영체제 내에서 물리적 메모리에 대한 I/O 작업을 하지 않고, 잔여 메모리를 페이지 캐시로 유지하여 성능향상을 도모하는데, 카프카는 이 페이지캐시를 이용하도록 설계되었음.
    - 때문에, 고성능 디스크 말고도 SATA와 같은 저성능 디스크를 사용해도 됨.
3. 배치 전송 처리
    - 데이터를 주고 받는 과정의 I/O 작업을 최소화하기 위해 묶어서 처리하는 배치방식 사용.
    - 메시지 보내는 시간을 1초라고 하면, 메시지 4개 = 4초 / 1번에 처리 : 4개 = 1초

카프카 데이터 모델
---
카프카가 고성능, 고가용성 메시징 애플리케이션으로 발전한 데에는 `토픽(topics)`과 `파티션(partitions)`의 역할이 컸음.

1. 토픽 
    - 우리가 원하는 데이터(`토픽`)를 저장하는 저장소 개념. 주소.  
    - 각자 데이터 용도나 목적에 맞는 네이밍을 통해서 구분을 해주어야함.
2. 파티션
    - 토픽의 분할개념. 
    - 하나로 저장하지 않고 분할하는 이유?
        - 메시징 큐 시스템의 제약조건 : 메시지의 순서가 보장되어야함.
        - 4개의 메시지가 있을때, 프로듀셔와 파티션이 1개라면 4초가 소요되지만, 프로듀서와 파티션을 4개로 늘리면 1초 소요.
    - 파티션의 수는 무조건적으로 늘리는 것이 아니다.
        - 파일 핸들링 증가 :
            - 파티션은 브로커의 디렉토리와 매핑되고, 각각 인덱스 파일과 실제 데이터파일이 존재함. 파티션이 많아지면 이 파일을에 대한 핸들 수가 증가하여 리소스를 낭비함.
        - 장애 복구 시간 증가 :
            - 브로커 > 토픽(N) > 파티션(N*M) 의 계층구조로 구성되어 있는데, 여기에 각 파티션별로 리플리케이션이 작동하여 하나는 리더로, 나머지는 팔로워가 된다.
            - 리더가 있는 파티션의 브로커가 장애가 발생하면, 해당 리더를 다른 브로커의 파티션으로 옮겨주어야하기 떄문에 파티션이 늘어나면 해당 문제를 처리하는 시간이 길어진다.
            - 또한 만약 해당 브로커가 컨트롤러인 경우까지 합쳐지면, 새 컨트롤러로 옮기는 시간동안 주키퍼 데이터 읽기 과정까지 소요되어 시간이 더 늘어나게 된다.
    - 최적의 파티션 수는 프로듀서와 컨슈머의 처리량로 판단
3. 오프셋과 메시지 순서
    - 파티션 내에 메시지가 저장되는 위치를 오프셋이라 하며 정수(int64)로 되어있으며, 파티션 내에서만 유일한 숫자로 구성.
    - 오프셋을 통해 메시지의 순서를 보장함.
    
카프카의 고가용성과 리플리케이션 
---
카프카의 고가용성: 서버의 물리적 장애가 발생하는 경우도 보장 가능. 이를 위해 리플리케이션을 도입

1. 리플리케이션 팩터, 리더/팔로워의 역할
- 리플리케이션 팩터 : 리플리케이션을 몇개를 만들것인지에 대한 설정

```shell script
vi /usr/local/kafka/config/server.properties 
```
```shell script
...
default.replication.factor = 2
...
```

```shell script
# config 변경사항 확인
cat /usr/local/kafka/logs/server.log
```

- 리플리케이션: 토픽 자체에 대한 리플리케이션을 만드는 것이 아니라, 각각의 파티션을 리플리케이션으로 만듦.
    - 리플리케이션을 사용하는 시스템의 경우 원본과 복제본을 다른 용어로 부름. eg)리더/팔로워(주키퍼, 카프카) , 마스터큐/미러드큐(래빗앰큐)
    - 데이터의 읽기/쓰기는 리더에 의해서만 일어남. 팔로워는 단순히 데이터만 리플리케이션만 함.
        
```shell script
# 카프카 토픽 생성
/usr/local/kafka/bin/kafka-topics.sh \
--zookeeper z1:2181,z2:2181,z3:2181,z4:2181,z5:2181/kafka \
--topic test --partitions 1 --replication-factor 2 --create
```

```shell script
# 카프카 토픽 생성정보 확인
/usr/local/kafka/bin/kafka-topics.sh \
--zookeeper z1:2181,z2:2181,z3:2181,z4:2181,z5:2181/kafka \
--topic test --describe
```

- 리플리케이션의 단점
    - 리플리케이션은 토픽(파티션)의 사이즈를 복제하는 것이기 때문에, 사이즈가 크면 리플리케이션 숫자만큼 배수로 증가하게 됨.
    - 리플리케이션이 잘 구성되어있는지 확인하는 작업을 위해 리소스가 투입되어야함.
    
2. 리더와 팔로워의 관리
- 리더 : 모든 데이터의 읽기 쓰기에 대한 요청에 응답하며 데이터를 저장
- 팔로워 : 리더를 주기적으로 바라보면서 자신에게 없는 데이터를 리더로 부터 가져와 저장함.
    - 이 과정에서, 팔로워가 리더로부터 데이터를 맞게 가져오지 못하면 정합성의 문제가 발생함.

#### ISR(In Sync Replica)
리더와 팔로워의 데이터 동기화 작업이 매우 중요하기 때문에 이것을 유지하기 위해 도입한 개념
- 현재 리플리케이션 되고 있는 리플리케이션 그룹을 말함.
- 리더의 경우 ISR의 구성원만이 자격을 가짐.

3. 모든 브로커에서 장애가 발생한 경우
- 고려할 수 있는 최선의 선택
    1. 마지막 리더가 살아나기를 기다린다.
    2. 장애 발생 과정에서 ISR에서 추방되었지만 자동으로 다시 살아나 리더가 된다.
    
- 1번의 경우 가장 안정적으로 데이터 손실이 없다는 장점이 있지만, 마지막 리더가 살아난다는 보장을 못해 장애가 길어질 수 있음
- 2번의 경우 시스템 정상화가 빠르지만, 데이터의 손실이 발생함
- 두 방법의 장단이 존재하기 때문에 무엇이 더 우수한지는 말할 수 없으며 설정을 통해서 할 수 있음
```shell script
vim /usr/local/kafka/config/server.properties
```

```shell script
...
unclean.leader.election.enable = true
...
```
- 여기서 true = 2번 방식, false = 1번 방식

주키퍼 지노드의 역할 
---
- 주키퍼 CLI 사용
```shell script
/usr/local/zookeeper/bin/zkCli.sh
```    

- 주키퍼 지노드 중요 역할 리스트
    - 컨트롤러: 카프카 클러스터의 컨트롤러, 리더를 선정하는 프로세스를 관장.  
    - 브로커: 브로커와 관련된 정보들이 저장됨. 초기 config 설정에서 broker.id 로 지노드를 작성. 주키퍼 임시노드를 작성하여 시작되며 브로커가 종료되면 사라지며, 클러스터 내 토픽정보 역시 확인 가능함(--describe)
    - 컨슈머: 컨슈머와 관련된 정보로, 컨슈머가 각각 파티션에서 어느 오프셋까지 읽었는지에 대한 정보를 저장. 이는 중요 정보이기 떄문에 *영구노드*로 저장. 이 컨슈머 오프셋 저장정보는 추후 카프카 내에 저장하는 방식으로 영구적으로 변경될 예정.
    - config: 토픽의 상세 생성정보 
  






 